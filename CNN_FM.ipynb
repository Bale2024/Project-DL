{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUydlx_7wDI_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(0.5,))])"
      ],
      "metadata": {
        "id": "CDl1v3hNxKdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers= 2)\n",
        "test_loader  = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=False, num_workers= 2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGyY2nqc2J8U",
        "outputId": "eabb0994-eaf7-4e60-b482-650e0103af62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:08<00:00, 3.30MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 305kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 2.31MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 23.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LeNet-5 Model\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self,num_classes=10):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "hpYIYMZL2uiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "K1Z1aiWj4dOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "  scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "  train_loss_list =[]\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    scheduler.step()\n",
        "# Generating Loss Curve:\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_loss_list.append(train_loss)\n",
        "    print(f'[Epoch {epoch + 1}/{(num_epochs)}], Loss:{train_loss:.4f}')\n",
        "\n",
        "# Model Testing\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images,labels in test_loader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  # Calculating Accuracy:\n",
        "  accuracy = 100 * correct / total\n",
        "  #Evaluation Metrics\n",
        "  precision = precision_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='macro')\n",
        "  recall = recall_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='macro')\n",
        "  f1 = f1_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='macro')\n",
        "\n",
        "  print(f'\\nAccuracy of the network : {accuracy:.2f}%')\n",
        "  print(f'Precision: {precision:.2f}')\n",
        "  print(f'Recall: {recall:.2f}')\n",
        "  print(f'F1-score: {f1:.2f}')\n",
        "\n",
        "  metrics = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1}\n",
        "  return metrics, train_loss_list"
      ],
      "metadata": {
        "id": "70I4Jkih4B92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride=1):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias =False)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias =False)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.shortcut = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride,bias =False),\n",
        "                                    nn.BatchNorm2d(out_channels))\n",
        "  def forward(self,x):\n",
        "    out = torch.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = torch.relu(out)\n",
        "    return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_blocks, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels = 64\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias =False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "    self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "  def  _make_layer(self,block,out_channels, num_blocks, stride):\n",
        "    strides = [stride] + [1]*(num_blocks-1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_channels, out_channels, stride))\n",
        "      self.in_channels = out_channels\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = torch.mean(out, dim=[2,3])\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "def ResNet18():\n",
        "  return ResNet(ResidualBlock, [2,2,2,2])\n",
        "net = ResNet18()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "79TqGbCYQ-G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Alexnet(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(Alexnet, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(256*3*3, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(4096, num_classes),\n",
        "    )\n",
        "     def forward(self,x):\n",
        "    x = self.features(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "4Yo5-3AaS5gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels, bias=False)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class Xception(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Xception, self).__init__()\n",
        "        self.entry_flow = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.middle_flow = nn.Sequential(\n",
        "            SeparableConv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            SeparableConv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            SeparableConv2d(256, 728, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(728),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.exit_flow = nn.Sequential(\n",
        "            SeparableConv2d(728, 1024, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.entry_flow(x)\n",
        "        x = self.middle_flow(x)\n",
        "        x = self.exit_flow(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "xception = Xception(num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "nATTJuePVjBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model_1 = LeNet5(num_classes=10).to(device)\n",
        "optimizer = optim.Adam(model_1.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "train_loss_list_1, metrics_1 = train_and_test(model_1, train_loader, test_loader, criterion, scheduler, optimizer, 20, device)\n",
        "with open('./train_loss_list_1.pkl', 'wb') as file:\n",
        "    pickle.dump(train_loss_list_1, file)\n",
        "with open('./metrics_1.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_1, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5R1B7mXmxR8",
        "outputId": "62b45e24-c19c-43ce-c3f4-2b7a635353d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/20], Loss:2.2880\n",
            "[Epoch 2/20], Loss:1.7662\n",
            "[Epoch 3/20], Loss:0.9418\n",
            "[Epoch 4/20], Loss:0.7772\n",
            "[Epoch 5/20], Loss:0.6899\n",
            "[Epoch 6/20], Loss:0.6372\n",
            "[Epoch 7/20], Loss:0.6301\n",
            "[Epoch 8/20], Loss:0.6240\n",
            "[Epoch 9/20], Loss:0.6185\n",
            "[Epoch 10/20], Loss:0.6127\n",
            "[Epoch 11/20], Loss:0.6080\n",
            "[Epoch 12/20], Loss:0.6074\n",
            "[Epoch 13/20], Loss:0.6067\n",
            "[Epoch 14/20], Loss:0.6063\n",
            "[Epoch 15/20], Loss:0.6058\n",
            "[Epoch 16/20], Loss:0.6053\n",
            "[Epoch 17/20], Loss:0.6052\n",
            "[Epoch 18/20], Loss:0.6052\n",
            "[Epoch 19/20], Loss:0.6051\n",
            "[Epoch 20/20], Loss:0.6050\n",
            "\n",
            "Accuracy of the network : 76.40%\n",
            "Precision: 0.80\n",
            "Recall: 0.80\n",
            "F1-score: 0.80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model_2 = ResNet18().to(device)\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "train_loss_list_2, metrics_2 = train_and_test(model_2, train_loader, test_loader, criterion, scheduler, optimizer, 20, device)\n",
        "with open('./train_loss_list_2.pkl', 'wb') as file:\n",
        "    pickle.dump(train_loss_list_2, file)\n",
        "with open('./metrics_2.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_2, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODTEaTW6Bm3D",
        "outputId": "de6a7971-5937-4f4e-d448-f5db8f1aca72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/20], Loss:0.5479\n",
            "[Epoch 2/20], Loss:0.2678\n",
            "[Epoch 3/20], Loss:0.2012\n",
            "[Epoch 4/20], Loss:0.1590\n",
            "[Epoch 5/20], Loss:0.1153\n",
            "[Epoch 6/20], Loss:0.0569\n",
            "[Epoch 7/20], Loss:0.0372\n",
            "[Epoch 8/20], Loss:0.0294\n",
            "[Epoch 9/20], Loss:0.0239\n",
            "[Epoch 11/20], Loss:0.0171\n",
            "[Epoch 12/20], Loss:0.0168\n",
            "[Epoch 13/20], Loss:0.0164\n",
            "[Epoch 14/20], Loss:0.0162\n",
            "[Epoch 15/20], Loss:0.0160\n",
            "[Epoch 16/20], Loss:0.0158\n",
            "[Epoch 17/20], Loss:0.0158\n",
            "[Epoch 18/20], Loss:0.0154\n",
            "[Epoch 19/20], Loss:0.0158\n",
            "[Epoch 20/20], Loss:0.0157\n",
            "\n",
            "Accuracy of the network : 92.36%\n",
            "Precision: 0.80\n",
            "Recall: 0.80\n",
            "F1-score: 0.80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model_3 = Alexnet().to(device)\n",
        "optimizer = optim.Adam(model_3.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "train_loss_list_3, metrics_3 = train_and_test(model_3, train_loader, test_loader, criterion, scheduler, optimizer, 20, device)\n",
        "with open('./train_loss_list_3.pkl', 'wb') as file:\n",
        "    pickle.dump(train_loss_list_3, file)\n",
        "with open('./metrics_3.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_3, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfPyYiQ9ILQj",
        "outputId": "c15aafb9-1448-4472-a7ed-3ad6aa841550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/20], Loss:2.3019\n",
            "[Epoch 2/20], Loss:2.2990\n",
            "[Epoch 3/20], Loss:2.2662\n",
            "[Epoch 4/20], Loss:1.3032\n",
            "[Epoch 5/20], Loss:0.8101\n",
            "[Epoch 6/20], Loss:0.7226\n",
            "[Epoch 7/20], Loss:0.7122\n",
            "[Epoch 8/20], Loss:0.7037\n",
            "[Epoch 9/20], Loss:0.6976\n",
            "[Epoch 10/20], Loss:0.6919\n",
            "[Epoch 11/20], Loss:0.6880\n",
            "[Epoch 12/20], Loss:0.6837\n",
            "[Epoch 13/20], Loss:0.6822\n",
            "[Epoch 14/20], Loss:0.6822\n",
            "[Epoch 15/20], Loss:0.6813\n",
            "[Epoch 16/20], Loss:0.6814\n",
            "[Epoch 17/20], Loss:0.6822\n",
            "[Epoch 18/20], Loss:0.6809\n",
            "[Epoch 19/20], Loss:0.6817\n",
            "[Epoch 20/20], Loss:0.6802\n",
            "\n",
            "Accuracy of the network : 73.85%\n",
            "Precision: 0.56\n",
            "Recall: 0.59\n",
            "F1-score: 0.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model_4 = Xception().to(device)\n",
        "optimizer = optim.Adam(model_4.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "train_loss_list_4, metrics_4 = train_and_test(model_4, train_loader, test_loader, criterion, scheduler, optimizer, 20, device)\n",
        "with open('./train_loss_list_4.pkl', 'wb') as file:\n",
        "    pickle.dump(train_loss_list_4, file)\n",
        "with open('./metrics_4.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_4, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPuMk_eWMOPL",
        "outputId": "0383b065-3206-47ca-ebe8-abef742de94d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/20], Loss:1.2091\n",
            "[Epoch 2/20], Loss:0.6931\n",
            "[Epoch 3/20], Loss:0.5673\n",
            "[Epoch 4/20], Loss:0.4947\n",
            "[Epoch 5/20], Loss:0.4421\n",
            "[Epoch 6/20], Loss:0.3984\n",
            "[Epoch 7/20], Loss:0.3912\n",
            "[Epoch 8/20], Loss:0.3858\n",
            "[Epoch 9/20], Loss:0.3806\n",
            "[Epoch 10/20], Loss:0.3770\n",
            "[Epoch 11/20], Loss:0.3725\n",
            "[Epoch 12/20], Loss:0.3710\n",
            "[Epoch 13/20], Loss:0.3709\n",
            "[Epoch 14/20], Loss:0.3699\n",
            "[Epoch 15/20], Loss:0.3698\n",
            "[Epoch 16/20], Loss:0.3694\n",
            "[Epoch 17/20], Loss:0.3692\n",
            "[Epoch 18/20], Loss:0.3689\n",
            "[Epoch 19/20], Loss:0.3693\n",
            "[Epoch 20/20], Loss:0.3692\n",
            "\n",
            "Accuracy of the network : 86.15%\n",
            "Precision: 0.78\n",
            "Recall: 0.85\n",
            "F1-score: 0.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}