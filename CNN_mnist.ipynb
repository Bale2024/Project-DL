{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56o6lDgsOTFj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the MNIST dataset\n",
        "])"
      ],
      "metadata": {
        "id": "WcY1iZvhO5Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "74wNddIdP4kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa8c976-7a8e-4450-8697-54ce5a127349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5025306.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 133755.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:06<00:00, 243520.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3027257.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "     if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "DKsy6uo4eTt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs, device):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "  train_loss_list = []\n",
        "\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "    scheduler.step()\n",
        "  # Generating Loss Curve\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_loss_list.append({'train_loss':train_loss})\n",
        "    print(f'[Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
        "\n",
        "  # Test the model\n",
        "\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      images, labels = data\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  # Calculating Accuracy\n",
        "  accuracy = 100 * correct / total\n",
        "\n",
        "  #Evaluation Metrics\n",
        "  precision = precision_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='macro')\n",
        "  recall = recall_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='macro')\n",
        "  f1 = f1_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='macro')\n",
        "\n",
        "  print(f'Accuracy of the network : {accuracy:.2f}%')\n",
        "  print(f'Precision: {precision:.2f}')\n",
        "  print(f'Recall: {recall:.2f}')\n",
        "  print(f'F1-score: {f1:.2f}')\n",
        "\n",
        "  metrics = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1}\n",
        "  return metrics, train_loss_list"
      ],
      "metadata": {
        "id": "s-Y9nmwXepNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Lenet_5(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(Lenet_5, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "    self.fc1 = nn.Linear(16*4*4, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.pool(torch.relu(self.conv1(x)))\n",
        "    x = self.pool(torch.relu(self.conv2(x)))\n",
        "    x = x.view(-1, 16*4*4)\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Jn7ZgAXQeFgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.shortcut = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                                    nn.BatchNorm2d(out_channels))\n",
        "  def forward(self,x):\n",
        "    out = torch.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = torch.relu(out)\n",
        "    return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_blocks, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels = 64\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias = False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "    self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "  def  _make_layer(self,block,out_channels, num_blocks, stride):\n",
        "    strides = [stride] + [1]*(num_blocks-1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_channels, out_channels, stride))\n",
        "      self.in_channels = out_channels\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = torch.mean(out, dim=[2,3])\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "def ResNet18():\n",
        "  return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "net = ResNet18()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Za_cjpR3en3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Alexnet(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(Alexnet, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(256*3*3, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(4096, num_classes),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x = self.features(x)\n",
        "    x = x.view(x.size(0), 256*3*3)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "9RiplWKGiVyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VGGnet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGGnet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(128,256, kernel_size=3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(256*3*3, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, num_classes)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = torch.relu(self.conv1(x))\n",
        "      x = torch.relu(self.conv2(x))\n",
        "      x = self.pool(x)\n",
        "\n",
        "      x = torch.relu(self.conv3(x))\n",
        "      x = torch.relu(self.conv4(x))\n",
        "      x = self.pool(x)\n",
        "\n",
        "      x = torch.relu(self.conv5(x))\n",
        "      x = torch.relu(self.conv6(x))\n",
        "      x = self.pool(x)\n",
        "\n",
        "      x = x.view(x.size(0), -1)\n",
        "      x = torch.relu(self.fc1(x))\n",
        "      x = torch.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "ZFo9hg7OiYdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model_1 = Lenet_5(num_classes=10).to(device)\n",
        "optimizer = optim.Adam(model_1.parameters(), lr=0.001)\n",
        "train_loss_list_1, metrics_1 = train_and_test(model_1, train_loader, test_loader, criterion, optimizer, 20, device)\n",
        "with open('./train_loss_list_1.pkl', 'wb') as file:\n",
        "    pickle.dump(train_loss_list_1, file)\n",
        "with open('./metrics_1.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_1, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d_NEnv2pxQt",
        "outputId": "c8afa7b9-9e33-4ed1-d3b7-8608246cb4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch [1/20], Loss: 0.1975\n",
            "[Epoch [2/20], Loss: 0.0676\n",
            "[Epoch [3/20], Loss: 0.0490\n",
            "[Epoch [4/20], Loss: 0.0402\n",
            "[Epoch [5/20], Loss: 0.0312\n",
            "[Epoch [6/20], Loss: 0.0285\n",
            "[Epoch [7/20], Loss: 0.0226\n",
            "[Epoch [8/20], Loss: 0.0198\n",
            "[Epoch [9/20], Loss: 0.0187\n",
            "[Epoch [10/20], Loss: 0.0164\n",
            "[Epoch [11/20], Loss: 0.0059\n",
            "[Epoch [12/20], Loss: 0.0030\n",
            "[Epoch [13/20], Loss: 0.0020\n",
            "[Epoch [14/20], Loss: 0.0015\n",
            "[Epoch [15/20], Loss: 0.0011\n",
            "[Epoch [16/20], Loss: 0.0009\n",
            "[Epoch [17/20], Loss: 0.0006\n",
            "[Epoch [18/20], Loss: 0.0004\n",
            "[Epoch [19/20], Loss: 0.0003\n",
            "[Epoch [20/20], Loss: 0.0004\n",
            "Accuracy of the network : 99.11%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model_2 = ResNet(BasicBlock,[2,2,2,2],num_classes=10).to(device)\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
        "train_loss_list_2, metrics_2 = train_and_test(model_2, train_loader, test_loader, criterion, optimizer, 20, device)\n",
        "with open('./train_loss_list_2.pkl', 'wb') as file:\n",
        "    pickle.dump(train_loss_list_2, file)\n",
        "with open('./metrics_2.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_2, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nycbNecaODbX",
        "outputId": "ae286d5b-3fc7-4d04-d797-7badf97b2290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch [1/20], Loss: 0.1113\n",
            "[Epoch [2/20], Loss: 0.0527\n",
            "[Epoch [3/20], Loss: 0.0383\n",
            "[Epoch [4/20], Loss: 0.0329\n",
            "[Epoch [5/20], Loss: 0.0255\n",
            "[Epoch [6/20], Loss: 0.0216\n",
            "[Epoch [7/20], Loss: 0.0169\n",
            "[Epoch [8/20], Loss: 0.0140\n",
            "[Epoch [9/20], Loss: 0.0121\n",
            "[Epoch [10/20], Loss: 0.0101\n",
            "[Epoch [11/20], Loss: 0.0033\n",
            "[Epoch [12/20], Loss: 0.0016\n",
            "[Epoch [13/20], Loss: 0.0010\n",
            "[Epoch [14/20], Loss: 0.0009\n",
            "[Epoch [15/20], Loss: 0.0005\n",
            "[Epoch [16/20], Loss: 0.0003\n",
            "[Epoch [17/20], Loss: 0.0002\n",
            "[Epoch [18/20], Loss: 0.0001\n",
            "[Epoch [19/20], Loss: 0.0001\n",
            "[Epoch [20/20], Loss: 0.0001\n",
            "Accuracy of the network : 99.55%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model_3 = Alexnet(num_classes=10).to(device)\n",
        "optimizer = optim.Adam(model_3.parameters(), lr=0.001)\n",
        "train_loss_list_3, metrics_3 = train_and_test(model_3, train_loader, test_loader, criterion, optimizer, 20, device)\n",
        "with open('./train_loss_list_3.pkl', 'wb') as file:\n",
        "    pickle.dump(train_loss_list_3, file)\n",
        "with open('./metrics_3.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_3, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jgvi6Ydl4DO",
        "outputId": "a7cb73e7-a24a-4de1-fbf7-e529896f83fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch [1/20], Loss: 0.2621\n",
            "[Epoch [2/20], Loss: 0.0963\n",
            "[Epoch [3/20], Loss: 0.0775\n",
            "[Epoch [4/20], Loss: 0.0670\n",
            "[Epoch [5/20], Loss: 0.0582\n",
            "[Epoch [6/20], Loss: 0.0502\n",
            "[Epoch [7/20], Loss: 0.0513\n",
            "[Epoch [8/20], Loss: 0.0448\n",
            "[Epoch [9/20], Loss: 0.0510\n",
            "[Epoch [10/20], Loss: 0.0462\n",
            "[Epoch [11/20], Loss: 0.0185\n",
            "[Epoch [12/20], Loss: 0.0105\n",
            "[Epoch [13/20], Loss: 0.0066\n",
            "[Epoch [14/20], Loss: 0.0050\n",
            "[Epoch [15/20], Loss: 0.0043\n",
            "[Epoch [16/20], Loss: 0.0030\n",
            "[Epoch [17/20], Loss: 0.0022\n",
            "[Epoch [18/20], Loss: 0.0026\n",
            "[Epoch [19/20], Loss: 0.0019\n",
            "[Epoch [20/20], Loss: 0.0015\n",
            "Accuracy of the network : 99.37%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model_4 = VGGnet(num_classes=10).to(device)\n",
        "optimizer = optim.Adam(model_4.parameters(), lr=0.001)\n",
        "train_loss_list_4, metrics_4 = train_and_test(model_4, train_loader, test_loader, criterion, optimizer, 20, device)\n",
        "with open('./train_loss_list_4.pkl', 'wb') as file:\n",
        "    pickle.dump(train_loss_list_4, file)\n",
        "with open('./metrics_4.pkl', 'wb') as file:\n",
        "    pickle.dump(metrics_4, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGH-HAfe2MQq",
        "outputId": "bf3ac6f3-5865-415b-b8c9-98e6cb7ffa00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch [1/20], Loss: 0.1727\n",
            "[Epoch [2/20], Loss: 0.0640\n",
            "[Epoch [3/20], Loss: 0.0472\n",
            "[Epoch [4/20], Loss: 0.0411\n",
            "[Epoch [5/20], Loss: 0.0365\n",
            "[Epoch [6/20], Loss: 0.0334\n",
            "[Epoch [7/20], Loss: 0.0318\n",
            "[Epoch [8/20], Loss: 0.0279\n",
            "[Epoch [9/20], Loss: 0.0291\n",
            "[Epoch [10/20], Loss: 0.0270\n",
            "[Epoch [11/20], Loss: 0.0083\n",
            "[Epoch [12/20], Loss: 0.0028\n",
            "[Epoch [13/20], Loss: 0.0012\n",
            "[Epoch [14/20], Loss: 0.0005\n",
            "[Epoch [15/20], Loss: 0.0002\n",
            "[Epoch [16/20], Loss: 0.0001\n",
            "[Epoch [17/20], Loss: 0.0000\n",
            "[Epoch [18/20], Loss: 0.0000\n",
            "[Epoch [19/20], Loss: 0.0001\n",
            "[Epoch [20/20], Loss: 0.0007\n",
            "Accuracy of the network : 99.42%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-score: 1.00\n"
          ]
        }
      ]
    }
  ]
}